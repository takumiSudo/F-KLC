# Scraper for Al Jazeera News

```{r}
# load relevant libraries
library(rvest)
library(tibble)
library(tidyverse)
```

```{r}
findurls <- function(baseurl) {
  homepage <- read_html(url)
  urls <- homepage |> 
    html_elements('.u-clickable-card__link') |>   
    html_attr("href")
  titles <- homepage |> 
    html_elements('.u-clickable-card__link') |>   
    html_text2()
  dates <- homepage |> 
    html_elements('.gc__date__date span[aria-hidden="true"]') |> 
    html_text2()
  # combine and filter out liveblogs
  all <- tibble(date=dates, title=titles, url=urls) |> 
    filter(!grepl("liveblog", url))
  return(all)
}
```

```{r}
findtext <- function(url) {
  newspage = read_html(url)
  text <- newspage |> 
    html_elements("#main-content-area p") |> 
    html_text2() |>
    str_c(collapse = " ")
  return(tibble(url=url, text=text))
}
```

```{r}
#setup base url
url <- "https://www.aljazeera.com/tag/israel-palestine-conflict/"

#find urls from homepage and complete urls
all_urls = findurls(url) |> 
  mutate(url = str_c("https://www.aljazeera.com", url))

head(all_urls)
```

```{r}
# find texts from urls
texts <- map(all_urls$url, findtext, .progress=TRUE) |> 
  list_rbind()
```

```{r}
# combine texts with existing frame
complete_frame <- inner_join(all_urls, texts, join_by(url))
```

# Second way to find urls

Automizing the clicking of the "Show More" Button

```{r}
automizeurl = "https://www.aljazeera.com/graphql?wp-site=aje&operationName=ArchipelagoAjeSectionPostsQuery&variables=%7B%22category%22%3A%22israel-palestine-conflict%22%2C%22categoryType%22%3A%22tags%22%2C%22postTypes%22%3A%5B%22blog%22%2C%22episode%22%2C%22opinion%22%2C%22post%22%2C%22video%22%2C%22external-article%22%2C%22gallery%22%2C%22podcast%22%2C%22longform%22%2C%22liveblog%22%5D%2C%22quantity%22%3A10%2C%22offset%22%3A14%7D&extensions=%7B%7D"
```
