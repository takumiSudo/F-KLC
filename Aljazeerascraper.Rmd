---
title: "Scraper Al Jazeera"
author: "Luuk Boekestein"
date: "2023-11-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# SCRAPER FOR AL JAZEERA

### Load packages

```{r}
library(rvest)
library(tibble)
library(tidyverse)
library(httr)
library(purrr)
library(dplyr)
library(stringr)
```

### Set up variables

```{r}
# specify site header
headers <- c("wp-site" = "aje")

# set number of articles to be scraped
max_articles <- 2000

# specify categories that are to be excluded
# - liveblog is short messages and parralel to general newsfeed and thus not useful
# - program is in video format, podcast is in audio format, therefore not useful
excluded_types <- list("liveblog", "program", "podcast")

# set how far back we want to scrape 
# (6th of October, since the conflict started on the 7th)
start_date <- parse_date_time("2023-10-6 12:00", "ymdHM")
```

# [Scraping Functions]{.underline}

## *Parsing Json format from fetch-url*

```{r}
parse_link_to_json <- function(url, headers) {
  # Make the HTTP request using httr
  response <- httr::GET(url, add_headers(headers))

  # Check if the request was successful
  if (httr::http_error(response)) {
    stop("HTTP request failed: ", httr::status_code(response))
  }

  # parse Json
  json_content <- httr::content(response, "text", encoding = "UTF-8")
  parsed_json <- jsonlite::fromJSON(json_content)
  
  return(parsed_json)
}
```

## *Scraping relevant data from Json*

```{r}
json_to_articles <- function(parsed_json) {
  #select subsection of Json
  articles_raw <- parsed_json$data$articles
  #select relevant elements
  articles <- articles_raw |> 
    select("title", 
           "date", 
           "link",
           "featuredImage"
           ) |>
    unnest_wider(c(featuredImage)) |>
    select(
      "title", 
      "date", 
      "link",
      "image_url" = "sourceUrl",
      "image_caption" = "caption",
      "image_alt" = "alt"
      ) |>
    mutate(date = parse_date_time(date, "ymdHMS")) |>
    filter(!str_detect(link, paste(excluded_types, collapse = "|")))
    
  return(articles)
}
```

## *Generating fetch-urls from desired number of article urls*

```{r}
generate_urls <- function(n_articles) {
  url_base <- "https://www.aljazeera.com/graphql?wp-site=aje&operationName=ArchipelagoAjeSectionPostsQuery&variables=%7B%22category%22%3A%22israel-palestine-conflict%22%2C%22categoryType%22%3A%22tags%22%2C%22postTypes%22%3A%5B%22blog%22%2C%22episode%22%2C%22opinion%22%2C%22post%22%2C%22video%22%2C%22external-article%22%2C%22gallery%22%2C%22podcast%22%2C%22longform%22%2C%22liveblog%22%5D%2C%22quantity%22%3A10%2C%22offset%22%3A"
  url_rest <- "%7D&extensions=%7B%7D"
  baseoffset <- 14
  
  offsets <- seq(baseoffset, by = 10, length.out = ceiling(n_articles/10))
  
  #generate urls
  urls <- lapply(offsets, function(offset) {
    paste0(url_base, offset, url_rest)
  })
  
  return(urls)
}
```

## *Retrieving n number of article urls*

```{r}
retrieve_articles <- function(n_articles, headers) {
  all_articles <- tibble()
  url_list <- generate_urls(n_articles)
  
  for(url in url_list) {
    json_data <- parse_link_to_json(url, headers)
    articles <- json_to_articles(json_data)
    all_articles <- bind_rows(all_articles, articles)
  }
  
  return(all_articles)
}
```

## *Retrieve article urls up to certain date*

```{r}
retrieve_articles_date <- function(max_articles, start_date, headers) {
  all_articles <- tibble()
  url_list <- generate_urls(max_articles)
  
  # set up progress bar
  pb <- progress_bar$new(total = length(url_list))
  
  for(url in url_list) {
    json_data <- parse_link_to_json(url, headers)
    articles <- json_to_articles(json_data)
    all_articles <- bind_rows(all_articles, articles)
    
    pb$tick()
    
    # stop if start date is reached
    if (min(all_articles$date) < start_date) {
      print("Reached starting date.")
      break
    }
  }
  
  all_articles <- all_articles |>
    filter(date > start_date)
   
  print(paste("Scraped",
              nrow(all_articles),
              "articles from", 
              min(all_articles$date),
              "to",
              max(all_articles$date)))
        
  return(all_articles)
}
```

## *Retrieving article text from url*

```{r}
retrieve_text <- function(url) {
  full_url = str_c("https://www.aljazeera.com", url)
  newspage = read_html(full_url)
  text <- newspage |> 
    html_elements("#main-content-area p") |> 
    html_text2() |>
    str_c(collapse = " ")
  return(tibble(link=url, text=text))
}
```

# Scraping Articles

### 1. Scrape the urls of the articles from the homepage

```{r}
article_urls <- retrieve_articles_date(max_articles, start_date, headers)
```

### 2. Scrape the text from the articles

```{r}
# find text
article_texts <- map(article_urls$link, retrieve_text, .progress=TRUE) |> 
  list_rbind()
```

### 3. Combine into one dataframe

```{r}
# combine into one dataframe
articles <- inner_join(article_urls, article_texts, join_by(link))
```

# Write results to csv

```{r}
write.csv(your_dataframe, "your_file.csv", row.names = FALSE)
```
